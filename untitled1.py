# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O_Tlk5PD6apoflVHkwo1m60uSU-wO6We
"""


import os
import sqlite3
import datetime
from sqlite3 import Error
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from ydata_profiling import ProfileReport
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import RandomForestRegressor
from dill import dump, load
import anvil.server
#from lazypredict import LazyClassifier

def load_csv(file_name):
  clean_texts =  lambda texts : [text.strip("\n").strip() for text in texts]
  with open(file_name, 'r') as f:
      header = clean_texts(f.readline().split(","))
      rows = [clean_texts(line.split(",")) for line in f.readlines()]
  return header,rows

def create_connection(db_file, delete_db=False):
    if delete_db and os.path.exists(db_file):
        os.remove(db_file)
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        conn.execute("PRAGMA foreign_keys = 1")
    except Error as e:
        print(e)
    return conn

conn = create_connection("normalised.db",delete_db=True)

def execute_sql_statement(sql_statement, conn):
    cur = conn.cursor()
    cur.execute(sql_statement)

    rows = cur.fetchall()

    return rows

def create_table(conn, create_table_sql, drop_table_name=None):

    if drop_table_name: # You can optionally pass drop_table_name to drop the table.
        try:
            c = conn.cursor()
            c.execute("""DROP TABLE IF EXISTS %s""" % (drop_table_name))
        except Error as e:
            print(e)

    try:
        c = conn.cursor()
        c.execute(create_table_sql)
    except Error as e:
        print(e)

def normalize_data(header,rows):
  output_rows = []
  for each in rows:
    entry = ()
    date_string = each[0]
    date_object = datetime.datetime.strptime(date_string, "%Y-%m-%d").date()
    open = round(float(each[1]),2)
    high = round(float(each[2]),2)
    low = round(float(each[3]),2)
    close = round(float(each[4]),2)
    adj_close = round(float(each[5]),2)
    volume = int(each[6])/1000
    if volume == 0.0:
      continue
      volume= 'Null'
    entry = (date_object,open,high,low,close,adj_close,volume)
    output_rows.append(entry)
  return output_rows

def write_google_data(conn,data_list):
  create_table(conn, """
        CREATE TABLE IF NOT EXISTS Google_Stocks (
            Date Date NOT NULL PRIMARY KEY,
            G_Open INTEGER NOT NULL,
            G_High INTEGER NOT NULL,
            G_Low INTEGER NOT NULL,
            G_Close INTEGER NOT NULL,
            G_AdjClose INTEGER NOT NULL,
            Google_Volume INTEGER NOT NULL
        );
    """, "Google_Stocks")

  cur = conn.cursor()
  cur.executemany("INSERT INTO Google_Stocks VALUES (?,?,?,?,?,?,?)", data_list)
  conn.commit()

def write_apple_data(conn,data_list):
  create_table(conn, """
        CREATE TABLE IF NOT EXISTS Apple_Stocks (
            Apple_Date Date NOT NULL PRIMARY KEY,
            A_Open INTEGER NOT NULL,
            A_High INTEGER NOT NULL,
            A_Low INTEGER NOT NULL,
            A_Close INTEGER NOT NULL,
            A_AdjClose INTEGER NOT NULL,
            Apple_Volume INTEGER NOT NULL
        );
    """, "Apple_Stocks")

  cur = conn.cursor()
  cur.executemany("INSERT INTO Apple_Stocks VALUES (?,?,?,?,?,?,?)", data_list)
  conn.commit()

"""**Data Pre-Processing**"""

header_goog,rows_goog= load_csv("GOOG.csv")
google_data_list = normalize_data(header_goog,rows_goog)
write_google_data(conn,google_data_list)


header_apple,rows_apple= load_csv("AAPL.csv")
apple_data_list = normalize_data(header_apple,rows_apple)
write_apple_data(conn,apple_data_list)

# SQL statement to fetch data from the database into a Pandas DataFrame. MUST USE JOINS.

sql_statement = """
SELECT *
FROM Google_Stocks AS GS
JOIN Apple_Stocks AS APS
ON GS.Date = APS.Apple_Date;
"""

df = pd.read_sql_query(sql_statement, conn)
del df['Apple_Date']


df['G_daily_price_change'] = round(abs(df['G_Open']-df['G_AdjClose']),2)
df['A_daily_price_change'] = round(abs(df['A_Open']-df['A_AdjClose']),2)

df['Stock with highest price range'] = df.apply(
    lambda row: 'Google' if row['G_daily_price_change'] > row['A_daily_price_change'] else 'Apple',
    axis=1
)

display(df)

x = df[["G_Open", "G_High","G_Close", "G_Low", "Google_Volume","A_Open", "A_High","A_Close", "A_Low", "Apple_Volume"]]
y = df["Stock with highest price range"]
# one hot encoding
y_encoded = y.factorize()
y = y_encoded[0]
labels = y_encoded[1]
print(labels)
# using the train test split function
X_train, X_test,y_train, y_test = train_test_split(x,y ,
                                   random_state=104,
                                   test_size=0.25,
                                   shuffle=True)

profile = ProfileReport(df, title="Profiling Report")
display(profile)

# NUll-check of the columns
df.isnull().sum()

# Utilzing Seaborn

corr = X_train.corr()
sns.heatmap(corr, annot=True)
plt.show()

# bar graph for y_train

count = df['Stock with highest price range'].value_counts()

plt.bar(count.index, count.values, color=['lightpink', 'lightblue'])
plt.title('Price Range')
plt.xlabel('Company')
plt.ylabel('Frequency')
plt.show()

# violin plots Display to demonstrate the correlation between the target value and categorical features.

for col in ['A_High','G_High']:
    sns.violinplot(y='Stock with highest price range', x=col, data=df)
    plt.show()

"""**Explanation-**
- **Categorical Heatmap:**
    - The heatmap shows the correlation between the categorical variable "Stock with highest price range" and the other categorical variables in the dataset.
    - The heatmap shows that there is a strong positive correlation between "Stock with highest price range" and "Google_Volume" and "Apple_Volume". This means that when the volume of Google or Apple stock is high, it is more likely that the stock with the highest price range will be Google or Apple, respectively.
- **Numerical Heatmap:**
    - The heatmap shows the correlation between the numerical variables in the dataset.
    - The heatmap shows that there is a strong positive correlation between "G_Open" and "G_High", "G_Open" and "G_Low", "G_High" and "G_Low", "A_Open" and "A_High", "A_Open" and "A_Low", and "A_High" and "A_Low". This means that when the opening price of Google or Apple stock is high, it is more likely that the high and low prices will also be high.
- **Insights:**
    - The analysis of the heatmaps suggests that the volume of Google and Apple stock is a good predictor of which stock will have the highest price range.
    - The analysis also suggests that the opening price of Google and Apple stock is a good predictor of the high and low prices of the stock.

These insights could be used to develop a trading strategy that takes into account the volume and opening price of Google and Apple stock when making trading decisions.

**Distribution of Numerical Features:**

- **G_Open:** The distribution of `G_Open` is slightly skewed to the right, indicating that there are a few days with unusually high opening prices.
- **G_High:** The distribution of `G_High` is also slightly skewed to the right, indicating that there are a few days with unusually high high prices.
- **G_Low:** The distribution of `G_Low` is slightly skewed to the left, indicating that there are a few days with unusually low low prices.
- **G_Close:** The distribution of `G_Close` is approximately normal, indicating that the closing prices are relatively evenly distributed.
- **Google_Volume:** The distribution of `Google_Volume` is highly skewed to the right, indicating that there are a few days with unusually high trading volume.
- **A_Open:** The distribution of `A_Open` is slightly skewed to the right, indicating that there are a few days with unusually high opening prices.
- **A_High:** The distribution of `A_High` is also slightly skewed to the right, indicating that there are a few days with unusually high high prices.
- **A_Low:** The distribution of `A_Low` is slightly skewed to the left, indicating that there are a few days with unusually low low prices.
- **A_Close:** The distribution of `A_Close` is approximately normal, indicating that the closing prices are relatively evenly distributed.
- **Apple_Volume:** The distribution of `Apple_Volume` is highly skewed to the right, indicating that there are a few days with unusually high trading volume.

**Insights:**

- The distributions of the opening, high, low, and closing prices for both Google and Apple stock are relatively similar.
- The trading volume for both Google and Apple stock is highly skewed, indicating that there are a few days with unusually high trading activity.
- These insights could be used to develop trading strategies that take into account the distribution of the opening, high, low, and closing prices, as well as the trading volume, when making trading decisions.

**Distribution of Categorical Feature:**

- **Stock with highest price range:** The distribution of `Stock with highest price range` shows that Google has a slightly higher frequency of having the highest price range compared to Apple.

**Insights:**

- This insight could be used to develop trading strategies that focus on buying the stock with the highest price range, which is more likely to be Google based on the historical data.

Overall, the analysis of the distribution of each attribute/feature provides valuable insights into the characteristics of Google and Apple stock prices and trading volume. These insights could be used to develop more informed trading strategies.
"""

class Preprocessor(BaseEstimator, TransformerMixin):
    # Train our custom preprocessors
    numerical_columns = [
        "G_Open",
        "G_High",
        "G_Close",
        "G_Low",
        "Google_Volume",
        "A_Open",
        "A_High",
        "A_Close",
        "A_Low",
        "Apple_Volume"
    ]
    categorical_columns = []

    def fit(self, X, y=None):

        # Create and fit simple imputer
        self.imputer = SimpleImputer(strategy='median')
        self.imputer.fit(X[self.numerical_columns])

        # Create and fit Standard Scaler
        self.scaler = StandardScaler()
        self.scaler.fit(X[self.numerical_columns])

        # Create and fit one hot encoder
        self.onehot = OneHotEncoder(handle_unknown='ignore')
        self.onehot.fit(X[self.categorical_columns])

        return self

    def transform(self, X):

        # Apply simple imputer
        imputed_cols = self.imputer.transform(X[self.numerical_columns])
        onehot_cols = self.onehot.transform(X[self.categorical_columns])

        # Copy the df
        transformed_df = X.copy()

        # Apply transformed columns
        transformed_df[self.numerical_columns] = imputed_cols
        transformed_df[self.numerical_columns] = self.scaler.transform(transformed_df[self.numerical_columns])

        # Drop existing categorical columns and replace with one hot equivalent
        transformed_df = transformed_df.drop(self.categorical_columns, axis=1)
        transformed_df[self.onehot.get_feature_names_out()] = onehot_cols.toarray().astype(int)

        return transformed_df

rfg = make_pipeline(Preprocessor(), RandomForestRegressor())

rfg.fit(X_train, y_train)

import json
print(json.dumps(X_test.iloc[0].to_dict(), indent=2))

from sklearn.metrics import mean_absolute_error
y_pred_train = rfg.predict(X_train)
mean_absolute_error(y_train, y_pred_train)

y_pred_test = rfg.predict(X_test)
mean_absolute_error(y_test, y_pred_test)

with open('rfg_model.pkl', 'wb') as f:
    dump(rfg, f)

with open('rfg_model.pkl', 'rb') as f:
    reloaded_model = load(f)

y_pred_test = reloaded_model.predict(X_test)
mean_absolute_error(y_test, y_pred_test)

df.head(1).to_dict()

payload = {
 'G_Open': 67.08,
 'G_High': 68.41,
 'G_Close': 68.37,
 'G_Low': 67.08,
 'Google_Volume': 28132.0,
 'A_Open': 74.06,
 'A_High': 75.15,
 'A_Close': 75.09,
 'A_Low': 73.8,
 'Apple_Volume': 135480.4
}
df = pd.DataFrame([payload.values()], columns=payload.keys())
df
a = reloaded_model.predict(df)
a

type(a[0])

'''from lazypredict import LazyClassifier

clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)
models,predictions = clf.fit(X_train, X_test, y_train, y_test)

print(models)'''



import mlflow
import dagshub
from sklearn.metrics import (
    accuracy_score,
    balanced_accuracy_score,
    f1_score,
    precision_score,
    recall_score,
    roc_auc_score,
    confusion_matrix
)
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis
from sklearn.calibration import CalibratedClassifierCV
from sklearn.model_selection import GridSearchCV
import numpy as np
import warnings

# Suppress all warnings
warnings.filterwarnings("ignore")

# Define the models to train
model_dict = {
    "QuadraticDiscriminantAnalysis": (QuadraticDiscriminantAnalysis(), {"reg_param": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]}),
    "LinearDiscriminantAnalysis": (LinearDiscriminantAnalysis(), {"solver": ["svd", "lsqr", "eigen"]}),
    "CalibratedClassifierCV": (CalibratedClassifierCV(), {"method": ["sigmoid", "isotonic"], "cv": [3, 5, 10]})
}

# Initialize DagsHub with MLflow integration
dagshub.init("my-first-repo", "ashutoshpanigrahi561", mlflow=True)
mlflow.start_run()


# feature engg
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

n_components = 10  # for example
pca = PCA(n_components=n_components)

pca.fit(X_train_scaled)

X_train = pca.transform(X_train_scaled)
X_test = pca.transform(X_test_scaled)

for model_name, (model, param_grid) in model_dict.items():
    with mlflow.start_run(nested=True):
        mlflow.log_param("model_name", model_name)

        try:
            # Perform GridSearchCV for hyperparameter tuning
            grid_search = GridSearchCV(model, param_grid, scoring='accuracy', cv=5)
            grid_search.fit(X_train, y_train)

            # Get the best model from grid search
            best_model = grid_search.best_estimator_
            best_params = grid_search.best_params_

            # Log the best hyperparameters
            mlflow.log_params(best_params)

            # Predict the labels using the best model
            y_pred = best_model.predict(X_test)
            y_prob = best_model.predict_proba(X_test)[:, 1] if hasattr(best_model, "predict_proba") else None

            # Calculate and log metrics
            accuracy = accuracy_score(y_test, y_pred)
            balanced_accuracy = balanced_accuracy_score(y_test, y_pred)
            f1 = f1_score(y_test, y_pred, average='weighted')
            precision = precision_score(y_test, y_pred, average='weighted')
            recall = recall_score(y_test, y_pred, average='weighted')
            roc_auc = roc_auc_score(y_test, y_prob) if y_prob is not None else None
            cm = confusion_matrix(y_test, y_pred)

            # Log metrics to MLflow
            mlflow.log_metric("accuracy", accuracy)
            mlflow.log_metric("balanced_accuracy", balanced_accuracy)
            mlflow.log_metric("f1_score", f1)
            mlflow.log_metric("precision", precision)
            mlflow.log_metric("recall", recall)
            if roc_auc is not None:
                mlflow.log_metric("roc_auc_score", roc_auc)
            mlflow.log_dict({"confusion_matrix": cm.tolist()}, "confusion_matrix.json")
        except Exception as e:
            print(f"An error occurred for model {model_name}: {e}")
            mlflow.log_param("error", str(e))

# End the MLflow run
mlflow.end_run()

# prompt: create a streamlit app

!streamlit run app.py

